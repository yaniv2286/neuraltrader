{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase2_header"
      },
      "source": [
        "# ğŸš€ PHASE 2: ADVANCED DEEP LEARNING MODELS\n",
        "## Hedge Fund Tier Trading System - GPU Training\n",
        "\n",
        "### ğŸ¯ Goals:\n",
        "- Train Transformer model (attention-based)\n",
        "- Train CNN-LSTM model (hybrid approach)\n",
        "- Create ensemble predictions\n",
        "- Target: 96%+ accuracy (better than 95.3% basic model)\n",
        "\n",
        "### â±ï¸ Timeline: Hours 4-8\n",
        "### ğŸ’° Investment: Free Colab GPU (testing first)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_gpu"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability and setup\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"ğŸš€ PHASE 2: ADVANCED DEEP LEARNING MODELS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"ğŸ”§ GPU Status: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"âœ… GPU Name: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"ğŸ”§ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "    print(f\"ğŸš€ Using GPU for training!\")\n",
        "else:\n",
        "    print(f\"âŒ No GPU - enable GPU in Runtime settings\")\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f\"ğŸ“Š Device: {device}\")\n",
        "print(f\"â° Started at: {datetime.now().strftime('%H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "print(f\"ğŸ“¦ Installing packages...\")\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers scikit-learn pandas numpy matplotlib seaborn yfinance ta\n",
        "print(f\"âœ… Packages installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "# Load ALL our Tiingo data (46 stocks)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(f\"ğŸ“ˆ Loading ALL our Tiingo data (46 stocks)...\")\n",
        "\n",
        "def load_tiingo_data(ticker):\n",
        "    \"\"\"Load Tiingo CSV data - supports all file naming patterns and locations\"\"\"\n",
        "    try:\n",
        "        # Search in multiple possible locations\n",
        "        search_paths = [\n",
        "            f\"data/cache/tiingo/{ticker}*.csv\",\n",
        "            f\"data/cache/{ticker}*.csv\",\n",
        "            f\"{ticker}*.csv\"  # Current directory as fallback\n",
        "        ]\n",
        "        \n",
        "        csv_files = []\n",
        "        for path in search_paths:\n",
        "            found = glob.glob(path)\n",
        "            csv_files.extend(found)\n",
        "        \n",
        "        if not csv_files:\n",
        "            print(f\"âŒ No CSV files found for {ticker}\")\n",
        "            return None\n",
        "        \n",
        "        # Use the first (or best) matching file\n",
        "        file_path = csv_files[0]\n",
        "        print(f\"ğŸ“ Found {ticker} file: {os.path.basename(file_path)}\")\n",
        "        \n",
        "        df = pd.read_csv(file_path)\n",
        "        \n",
        "        # Handle date column\n",
        "        date_col = None\n",
        "        for col in df.columns:\n",
        "            if col.lower() == 'date':\n",
        "                date_col = col\n",
        "                break\n",
        "            elif col.lower() == 'datetime':\n",
        "                date_col = col\n",
        "                break\n",
        "        \n",
        "        if date_col:\n",
        "            df[date_col] = pd.to_datetime(df[date_col])\n",
        "            df.set_index(date_col, inplace=True)\n",
        "        elif 'Date' in df.columns:\n",
        "            df['Date'] = pd.to_datetime(df['Date'])\n",
        "            df.set_index('Date', inplace=True)\n",
        "        \n",
        "        # Map columns to standard names\n",
        "        col_mapping = {}\n",
        "        for col in df.columns:\n",
        "            col_lower = col.lower()\n",
        "            if 'close' in col_lower and 'Close' not in df.columns:\n",
        "                col_mapping['Close'] = col\n",
        "            elif 'open' in col_lower and 'Open' not in df.columns:\n",
        "                col_mapping['Open'] = col\n",
        "            elif 'high' in col_lower and 'High' not in df.columns:\n",
        "                col_mapping['High'] = col\n",
        "            elif 'low' in col_lower and 'Low' not in df.columns:\n",
        "                col_mapping['Low'] = col\n",
        "            elif 'volume' in col_lower and 'Volume' not in df.columns:\n",
        "                col_mapping['Volume'] = col\n",
        "            elif 'adj close' in col_lower and 'Close' not in df.columns:\n",
        "                col_mapping['Close'] = col\n",
        "        \n",
        "        if col_mapping:\n",
        "            df = df.rename(columns=col_mapping)\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading {ticker}: {e}\")\n",
        "        return None\n",
        "\n",
        "# First, let's see what CSV files we actually have\n",
        "print(f\"ğŸ” Scanning for available CSV files...\")\n",
        "all_csv_files = []\n",
        "all_csv_files.extend(glob.glob(\"data/cache/tiingo/*.csv\"))\n",
        "all_csv_files.extend(glob.glob(\"data/cache/*.csv\"))\n",
        "\n",
        "print(f\"ğŸ“ Found {len(all_csv_files)} CSV files:\")\n",
        "for file in sorted(all_csv_files)[:10]:  # Show first 10\n",
        "    print(f\"   {os.path.basename(file)}\")\n",
        "if len(all_csv_files) > 10:\n",
        "    print(f\"   ... and {len(all_csv_files) - 10} more\")\n",
        "\n",
        "# Extract tickers from available files\n",
        "available_tickers = set()\n",
        "for file in all_csv_files:\n",
        "    ticker = os.path.basename(file).split('_')[0]  # Get ticker before first underscore\n",
        "    available_tickers.add(ticker)\n",
        "\n",
        "print(f\"\\nğŸ“Š Available tickers from files: {sorted(list(available_tickers))}\")\n",
        "\n",
        "# ALL 46 STOCKS we want to load\n",
        "all_stocks = [\n",
        "    # Technology (9 stocks)\n",
        "    'AAPL', 'MSFT', 'GOOGL', 'META', 'NVDA', 'AMD', 'INTC', 'CSCO', 'ORCL',\n",
        "    \n",
        "    # Financial (6 stocks)\n",
        "    'JPM', 'V', 'AXP', 'BAC', 'WFC', 'GS',\n",
        "    \n",
        "    # Healthcare (5 stocks)\n",
        "    'JNJ', 'UNH', 'PFE', 'MRK', 'ABT',\n",
        "    \n",
        "    # Consumer (12 stocks)\n",
        "    'AMZN', 'TSLA', 'DIS', 'WMT', 'PG', 'HD', 'MCD', 'NKE', 'BABA', 'LULU', 'EL', 'CALM',\n",
        "    \n",
        "    # Industrial (7 stocks)\n",
        "    'CAT', 'GE', 'HON', 'AAL', 'DAL', 'UAL', 'LUV',\n",
        "    \n",
        "    # Energy (2 stocks)\n",
        "    'XOM', 'CVX',\n",
        "    \n",
        "    # ETFs (5 stocks)\n",
        "    'QQQ', 'SPY', 'VXX', 'KLAC', 'EIS'\n",
        "]\n",
        "\n",
        "print(f\"\\nğŸ“Š Attempting to load {len(all_stocks)} stocks...\")\n",
        "\n",
        "stock_data = {}\n",
        "successful_loads = 0\n",
        "failed_loads = []\n",
        "\n",
        "for ticker in all_stocks:\n",
        "    data = load_tiingo_data(ticker)\n",
        "    if data is not None:\n",
        "        stock_data[ticker] = data\n",
        "        successful_loads += 1\n",
        "        print(f\"âœ… {ticker}: {len(data)} days loaded\")\n",
        "    else:\n",
        "        failed_loads.append(ticker)\n",
        "        print(f\"âŒ {ticker}: Failed to load\")\n",
        "\n",
        "print(f\"\\nğŸ“Š LOADING SUMMARY:\")\n",
        "print(f\"âœ… Successfully loaded: {successful_loads}/{len(all_stocks)} stocks\")\n",
        "print(f\"âŒ Failed to load: {len(failed_loads)} stocks\")\n",
        "\n",
        "if failed_loads:\n",
        "    print(f\"âŒ Failed stocks: {failed_loads}\")\n",
        "\n",
        "# Use GOOGL as our primary stock for training (best performer)\n",
        "if 'GOOGL' in stock_data:\n",
        "    googl_data = stock_data['GOOGL'].copy()\n",
        "    # Reset index to get Date as a column\n",
        "    googl_data.reset_index(inplace=True)\n",
        "    print(f\"\\nğŸ¯ Using GOOGL for training: {len(googl_data)} days\")\n",
        "    print(f\"\udcca Columns: {list(googl_data.columns)}\")\n",
        "    \n",
        "    # Check if we have a Date column after reset_index\n",
        "    if 'Date' in googl_data.columns:\n",
        "        print(f\"\ud83dğŸ“… Period: {googl_data['Date'].min().date()} to {googl_data['Date'].max().date()}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ Date column not found, available columns: {list(googl_data.columns)}\")\n",
        "else:\n",
        "    print(f\"\\nâŒ GOOGL data not found!\")\n",
        "    # Use first available stock\n",
        "    if stock_data:\n",
        "        first_stock = list(stock_data.keys())[0]\n",
        "        googl_data = stock_data[first_stock].copy()\n",
        "        googl_data.reset_index(inplace=True)\n",
        "        print(f\"ğŸ”„ Using {first_stock} instead: {len(googl_data)} days\")\n",
        "\n",
        "print(f\"\\nğŸ‰ PORTFOLIO STATUS:\")\n",
        "print(f\"ğŸ“Š Total stocks available: {len(stock_data)}\")\n",
        "print(f\"ğŸ“ˆ Stock list: {sorted(list(stock_data.keys()))}\")\n",
        "\n",
        "# Display sector breakdown\n",
        "sectors = {\n",
        "    'Technology': ['AAPL', 'MSFT', 'GOOGL', 'META', 'NVDA', 'AMD', 'INTC', 'CSCO', 'ORCL'],\n",
        "    'Financial': ['JPM', 'V', 'AXP', 'BAC', 'WFC', 'GS'],\n",
        "    'Healthcare': ['JNJ', 'UNH', 'PFE', 'MRK', 'ABT'],\n",
        "    'Consumer': ['AMZN', 'TSLA', 'DIS', 'WMT', 'PG', 'HD', 'MCD', 'NKE', 'BABA', 'LULU', 'EL', 'CALM'],\n",
        "    'Industrial': ['CAT', 'GE', 'HON', 'AAL', 'DAL', 'UAL', 'LUV'],\n",
        "    'Energy': ['XOM', 'CVX'],\n",
        "    'ETFs': ['QQQ', 'SPY', 'VXX', 'KLAC', 'EIS']\n",
        "}\n",
        "\n",
        "print(f\"\\nğŸ¢ SECTOR BREAKDOWN:\")\n",
        "for sector, stocks in sectors.items():\n",
        "    loaded = [s for s in stocks if s in stock_data]\n",
        "    print(f\"   {sector}: {len(loaded)}/{len(stocks)} loaded - {loaded}\")\n",
        "\n",
        "print(f\"\\nâœ… Data loading complete! Ready for feature engineering...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_features"
      },
      "outputs": [],
      "source": [
        "# Create enhanced features\n",
        "def create_enhanced_features(df):\n",
        "    features = pd.DataFrame(index=df.index)\n",
        "    \n",
        "    # Price-based features\n",
        "    for period in [5, 10, 20, 50]:\n",
        "        features[f'close_ma_{period}'] = df['Close'].rolling(period).mean()\n",
        "        features[f'close_std_{period}'] = df['Close'].rolling(period).std()\n",
        "        features[f'close_bb_position_{period}'] = (df['Close'] - features[f'close_ma_{period}']) / features[f'close_std_{period}']\n",
        "        features[f'close_ma_ratio_{period}'] = df['Close'] / features[f'close_ma_{period}']\n",
        "    \n",
        "    # Returns\n",
        "    for period in [1, 5, 10, 20]:\n",
        "        features[f'return_{period}'] = df['Close'].pct_change(period)\n",
        "        features[f'return_abs_{period}'] = abs(df['Close'].pct_change(period))\n",
        "    \n",
        "    # RSI\n",
        "    for period in [14, 30]:\n",
        "        delta = df['Close'].diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "        rs = gain / loss\n",
        "        features[f'rsi_{period}'] = 100 - (100 / (1 + rs))\n",
        "    \n",
        "    # MACD\n",
        "    ema_12 = df['Close'].ewm(span=12).mean()\n",
        "    ema_26 = df['Close'].ewm(span=26).mean()\n",
        "    features['macd'] = ema_12 - ema_26\n",
        "    features['macd_signal'] = features['macd'].ewm(span=9).mean()\n",
        "    \n",
        "    # Volume\n",
        "    features['volume_ratio'] = df['Volume'] / df['Volume'].rolling(20).mean()\n",
        "    \n",
        "    return features.dropna()\n",
        "\n",
        "features = create_enhanced_features(googl_data)\n",
        "print(f\"âœ… Enhanced features created: {features.shape[1]} features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_sequences"
      },
      "outputs": [],
      "source": [
        "# Create sequences for deep learning\n",
        "def create_sequences(features, target, sequence_length=30):\n",
        "    X, y = [], []\n",
        "    for i in range(sequence_length, len(features)):\n",
        "        X.append(features.iloc[i-sequence_length:i].values)\n",
        "        y.append(target.iloc[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Prepare target and sequences\n",
        "target = googl_data['Close'].shift(-1)\n",
        "features = features.dropna()\n",
        "target = target.dropna()\n",
        "common_idx = features.index.intersection(target.index)\n",
        "features = features.loc[common_idx]\n",
        "target = target.loc[common_idx]\n",
        "\n",
        "X, y = create_sequences(features, target, 30)\n",
        "\n",
        "# Proper chronological split: 70% train, 15% val, 15% test\n",
        "train_idx = int(0.7 * len(X))\n",
        "val_idx = int(0.85 * len(X))\n",
        "\n",
        "X_train, X_val, X_test = X[:train_idx], X[train_idx:val_idx], X[val_idx:]\n",
        "y_train, y_val, y_test = y[:train_idx], y[train_idx:val_idx], y[val_idx:]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
        "X_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
        "X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
        "\n",
        "print(f\"âœ… Sequences created:\")\n",
        "print(f\"   Train: {X_train_scaled.shape}\")\n",
        "print(f\"   Val: {X_val_scaled.shape}\")  \n",
        "print(f\"   Test: {X_test_scaled.shape}\")\n",
        "print(f\"   Target range: ${y_train.min():.2f} - ${y_train.max():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "define_models"
      },
      "outputs": [],
      "source": [
        "# Define deep learning models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_size, d_model=256, n_heads=8, num_layers=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_projection = nn.Linear(input_size, d_model)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1000, d_model) * 0.02)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=512, dropout=dropout, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.output_projection = nn.Sequential(\n",
        "            nn.Linear(d_model, 128), \n",
        "            nn.ReLU(), \n",
        "            nn.Dropout(dropout), \n",
        "            nn.Linear(128, 64), \n",
        "            nn.ReLU(), \n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.input_projection(x)\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.positional_encoding[:seq_len]\n",
        "        x = self.transformer(x)\n",
        "        x = x[:, -1, :]\n",
        "        x = self.output_projection(x)\n",
        "        return x\n",
        "\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=256, num_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # CNN layers for feature extraction\n",
        "        self.conv1d_1 = nn.Conv1d(input_size, 128, kernel_size=3, padding=1)\n",
        "        self.conv1d_2 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv1d_3 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # LSTM for temporal patterns\n",
        "        self.lstm = nn.LSTM(512, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        \n",
        "        # Output layers\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 128), \n",
        "            nn.ReLU(), \n",
        "            nn.Dropout(dropout), \n",
        "            nn.Linear(128, 64), \n",
        "            nn.ReLU(), \n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # CNN feature extraction\n",
        "        x = x.transpose(1, 2)  # (batch, features, seq_len)\n",
        "        x = torch.relu(self.conv1d_1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.conv1d_2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.conv1d_3(x))\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # LSTM processing\n",
        "        x = x.transpose(1, 2)  # (batch, seq_len, features)\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "        final_output = lstm_out[:, -1, :]  # Use last output\n",
        "        \n",
        "        # Final prediction\n",
        "        output = self.output(final_output)\n",
        "        return output\n",
        "\n",
        "print(f\"ğŸ¤– Improved models defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_functions"
      },
      "outputs": [],
      "source": [
        "# Training and evaluation functions\n",
        "def train_model(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=32, learning_rate=5e-5):\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "    y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
        "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
        "    y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    # Optimizer with conservative settings\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(), \n",
        "        lr=learning_rate,  # Much lower learning rate\n",
        "        weight_decay=1e-4,  # Add regularization\n",
        "        betas=(0.9, 0.999),\n",
        "        eps=1e-8\n",
        "    )\n",
        "    \n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, \n",
        "        max_lr=learning_rate,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.1,\n",
        "        anneal_strategy='cos'\n",
        "    )\n",
        "    \n",
        "    criterion = nn.MSELoss()\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    print(f\"ğŸ§  Training on {device}...\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(batch_X).squeeze()\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            \n",
        "            # Backward pass with gradient clipping\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            num_batches += 1\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val_tensor).squeeze()\n",
        "            val_loss = criterion(val_outputs, y_val_tensor).item()\n",
        "        \n",
        "        avg_train_loss = train_loss / num_batches\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        \n",
        "        # Print progress\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"   Epoch {epoch:3d}: Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "            \n",
        "            # Check for overfitting\n",
        "            if val_loss > avg_train_loss * 3:\n",
        "                print(f\"   âš ï¸ Overfitting detected!\")\n",
        "    \n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    print(f\"âœ… Training completed! Best val loss: {best_val_loss:.6f}\")\n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    model.eval()\n",
        "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X_test_tensor).squeeze().cpu().numpy()\n",
        "    \n",
        "    mse = np.mean((predictions - y_test) ** 2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    \n",
        "    # Better accuracy calculation\n",
        "    y_mean = np.mean(y_test)\n",
        "    accuracy = max(0, 1 - (rmse / y_mean))  # Ensure non-negative\n",
        "    \n",
        "    # Direction accuracy\n",
        "    direction_correct = np.mean(np.sign(predictions[:-1]) == np.sign(y_test[1:]))\n",
        "    \n",
        "    return {\n",
        "        'predictions': predictions, \n",
        "        'rmse': rmse, \n",
        "        'accuracy': accuracy,\n",
        "        'direction_accuracy': direction_correct\n",
        "    }\n",
        "\n",
        "print(f\"ğŸ¯ Improved training functions ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_transformer"
      },
      "outputs": [],
      "source": [
        "# Train Transformer model\n",
        "print(f\"ğŸ¤– Training Transformer model...\")\n",
        "transformer_model = TransformerModel(\n",
        "    input_size=X_train_scaled.shape[-1], \n",
        "    d_model=256, \n",
        "    n_heads=8, \n",
        "    num_layers=4, \n",
        "    dropout=0.1\n",
        ")\n",
        "transformer_model, transformer_train_losses, transformer_val_losses = train_model(\n",
        "    transformer_model, \n",
        "    X_train_scaled, y_train, \n",
        "    X_val_scaled, y_val, \n",
        "    epochs=50, \n",
        "    batch_size=32,\n",
        "    learning_rate=5e-5\n",
        ")\n",
        "transformer_results = evaluate_model(transformer_model, X_test_scaled, y_test)\n",
        "print(f\"ğŸ“Š Transformer Results:\")\n",
        "print(f\"   Accuracy: {transformer_results['accuracy']:.1%}\")\n",
        "print(f\"   Direction Accuracy: {transformer_results['direction_accuracy']:.1%}\")\n",
        "print(f\"   RMSE: ${transformer_results['rmse']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_cnn_lstm"
      },
      "outputs": [],
      "source": [
        "# Train CNN-LSTM model\n",
        "print(f\"ğŸ¤– Training CNN-LSTM model...\")\n",
        "cnn_lstm_model = CNNLSTMModel(\n",
        "    input_size=X_train_scaled.shape[-1], \n",
        "    hidden_size=256, \n",
        "    num_layers=2, \n",
        "    dropout=0.1\n",
        ")\n",
        "cnn_lstm_model, cnn_train_losses, cnn_val_losses = train_model(\n",
        "    cnn_lstm_model, \n",
        "    X_train_scaled, y_train, \n",
        "    X_val_scaled, y_val, \n",
        "    epochs=50, \n",
        "    batch_size=32,\n",
        "    learning_rate=5e-5\n",
        ")\n",
        "cnn_lstm_results = evaluate_model(cnn_lstm_model, X_test_scaled, y_test)\n",
        "print(f\"ğŸ“Š CNN-LSTM Results:\")\n",
        "print(f\"   Accuracy: {cnn_lstm_results['accuracy']:.1%}\")\n",
        "print(f\"   Direction Accuracy: {cnn_lstm_results['direction_accuracy']:.1%}\")\n",
        "print(f\"   RMSE: ${cnn_lstm_results['rmse']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ensemble_results"
      },
      "outputs": [],
      "source": [
        "# Create ensemble and compare\n",
        "ensemble_predictions = (transformer_results['predictions'] + cnn_lstm_results['predictions']) / 2\n",
        "ensemble_rmse = np.sqrt(np.mean((ensemble_predictions - y_test) ** 2))\n",
        "ensemble_accuracy = max(0, 1 - (ensemble_rmse / np.mean(y_test)))\n",
        "ensemble_direction_accuracy = np.mean(np.sign(ensemble_predictions[:-1]) == np.sign(y_test[1:]))\n",
        "\n",
        "basic_accuracy = 0.953  # From Phase 1\n",
        "improvement = ensemble_accuracy - basic_accuracy\n",
        "\n",
        "print(f\"\\nğŸ¯ PERFORMANCE COMPARISON:\")\n",
        "print(f\"   Basic Model (Random Forest): {basic_accuracy:.1%}\")\n",
        "print(f\"   Transformer: {transformer_results['accuracy']:.1%}\")\n",
        "print(f\"   CNN-LSTM: {cnn_lstm_results['accuracy']:.1%}\")\n",
        "print(f\"   Deep Learning Ensemble: {ensemble_accuracy:.1%}\")\n",
        "print(f\"   Improvement: {improvement:+.1%}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š DIRECTION ACCURACY:\")\n",
        "print(f\"   Transformer: {transformer_results['direction_accuracy']:.1%}\")\n",
        "print(f\"   CNN-LSTM: {cnn_lstm_results['direction_accuracy']:.1%}\")\n",
        "print(f\"   Ensemble: {ensemble_direction_accuracy:.1%}\")\n",
        "\n",
        "print(f\"\\nğŸ’° PREDICTION ERROR:\")\n",
        "print(f\"   Transformer RMSE: ${transformer_results['rmse']:.2f}\")\n",
        "print(f\"   CNN-LSTM RMSE: ${cnn_lstm_results['rmse']:.2f}\")\n",
        "print(f\"   Ensemble RMSE: ${ensemble_rmse:.2f}\")\n",
        "\n",
        "if ensemble_accuracy > basic_accuracy:\n",
        "    print(f\"\\nğŸ† SUCCESS! Deep learning ensemble is better!\")\n",
        "    print(f\"   ğŸ“ˆ Improvement: {improvement:+.1%}\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸  Basic model still performs better\")\n",
        "    print(f\"   ğŸ“‰ Gap: {basic_accuracy - ensemble_accuracy:.1%}\")\n",
        "\n",
        "# Plot training curves if you want to visualize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(transformer_train_losses, label='Transformer Train')\n",
        "plt.plot(transformer_val_losses, label='Transformer Val')\n",
        "plt.title('Transformer Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(cnn_train_losses, label='CNN-LSTM Train')\n",
        "plt.plot(cnn_val_losses, label='CNN-LSTM Val')\n",
        "plt.title('CNN-LSTM Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nğŸ‰ PHASE 2 COMPLETE!\")\n",
        "print(f\"â° Finished at: {datetime.now().strftime('%H:%M:%S')}\")\n",
        "print(f\"ğŸš€ Ready for Phase 3: Alternative Data Integration\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
